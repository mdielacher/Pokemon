{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for OpenAI Pokemon Namen Generierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Jupyter Notebook wurde entwickelt, um für jeden generierten synthetischen Datenpunkt einen passenden Pokémon-Namen zu generieren. Diese Generierung erfolgt mithilfe der OpenAI-Plattform und dient dazu, die Fähigkeiten von OpenAI im Bereich der kreativen Namensgebung zu demonstrieren.\n",
    "\n",
    "#### Hintergrund\n",
    "\n",
    "Die Generierung von Namen für synthetische Datenpunkte ist eine interessante Anwendung von KI-Technologien wie OpenAI. In diesem Notebook wird untersucht, wie gut OpenAI darin ist, Pokémon-Namen auf der Grundlage von gegebenen Statistiken zu erstellen. Dieser Ansatz ermöglicht es, automatisch und effizient eine große Anzahl von Namen zu generieren, die den vorgegebenen Kriterien entsprechen.\n",
    "\n",
    "#### Ziel\n",
    "\n",
    "Das Hauptziel dieses Notebooks besteht darin, die Qualität der von OpenAI generierten Pokémon-Namen zu bewerten. Hierbei wird auch ein Vergleich mit einem anderen LLM namens Llama 2 durchgeführt, um zu sehen, welches System bessere Namen erzeugt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api_keys from json\n",
    "with open('api_key.json') as json_datei:\n",
    "    keys = json.load(json_datei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = keys[\"open_ai_api_key\"] # set api key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama-index OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dieses Promptemaple definiert die Vorlage für die Generierung eines Pokémon-Namens auf der Grundlage der angegebenen Werte. (Sehr wichtig um Halluzination zu vermeiden!)\n",
    "# Stats include: Name, Type 1, Type 2, Total, HP, Attack, Defense, Sp. Atk, Sp. Def, Speed, Generation, Legendary, Evolution\n",
    "# Examples:\n",
    "# Input: Grass, Poison, 318, 45, 49, 49, 65, 65, 45, 1, False, 0; Output: Bulbasaur\n",
    "# Input: Fire, NaN, 309, 39, 52, 43, 60, 50, 65, 1, False, 1; Output: Charmander\n",
    "# Input: Fire, Water, 600, 80, 110, 120, 130, 90, 70, 6, True, 0; Output: Volcanion\n",
    "# Only the name is output.\n",
    "prompt_template= '<SYS> Generiere mir auf Basis dieser Stats einen Pokemon namen den es nicht gibt: Stats: Name, Type 1, Type 2, Total, HP, Attack, Defense, \tSp. Atk,\tSp. Def,\tSpeed,\tGeneration,\tLegendary, Evolution // Beispiele: Input:  Grass, Poison, 318, 45, 49,\t49,\t65,\t65,\t45,\t1,\tFalse, 0; Output: Bulbasaur Input: Fire, NaN,\t309,\t39,\t52,\t43,\t60,\t50,\t65\t1 False, 1; Output: Charmander Input: Fire, Water,\t600,\t80,\t110,\t120,\t130,\t90,\t70,\t6,\tTrue, 0; Output: Volcanion\t Gib mir nur den Namen aus!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_pokemon_assistant(pokemon_stats):\n",
    "    \"\"\" OpenAI Pokemon Assistant um Pokemon Namen anhand von einem String mit den Stats zu bewerten\"\"\"\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=prompt_template), # Systemnachricht mit dem vordefinierten prompt_template\n",
    "        ChatMessage(role=\"user\", content=pokemon_stats), # Pokemon_stats\n",
    "    ]\n",
    "    resp = OpenAI().chat(messages)\n",
    "    return resp.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datenvorbereinigung\n",
    "syn_data = pd.read_csv(\"Gaussian_Pokemon.csv\")\n",
    "del syn_data[\"Unnamed: 0\"]\n",
    "syn_data = syn_data[0:10]\n",
    "del syn_data[\"Name\"]\n",
    "\n",
    "# Wichtig um einen String zu generieren für das OpenAI Modul\n",
    "syn_data['Attribut'] = syn_data['Type 1'] + ', ' + syn_data['Type 2'].astype(str) + ', ' + syn_data['HP'].astype(str) + ', ' + \\\n",
    "                        syn_data['Attack'].astype(str) + ', ' + syn_data['Defense'].astype(str) + ', ' + syn_data['Sp. Atk'].astype(str) + \\\n",
    "                        ', ' + syn_data['Sp. Def'].astype(str) + ', ' + syn_data['Speed'].astype(str) + ', ' + \\\n",
    "                        syn_data['Generation'].astype(str) + ', ' + syn_data['Legendary'].astype(str) + ', ' + \\\n",
    "                        syn_data['Evolution'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data['Name_open_ai'] = syn_data[\"Attribut\"].apply(open_ai_pokemon_assistant) # Assistant wird für jeden Datenpunkt (Pokemon) angewendet.\n",
    "columns = ['Name_open_ai'] + [col for col in syn_data.columns if col != 'Name_open_ai'] # Name sollte ganz vorne stehen :)\n",
    "syn_data = syn_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_open_ai</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "      <th>Evolution</th>\n",
       "      <th>Attribut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragound</td>\n",
       "      <td>Ground</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>81</td>\n",
       "      <td>93</td>\n",
       "      <td>99</td>\n",
       "      <td>88</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ground, Dragon, 81, 93, 99, 88, 83, 71, 6, Fal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aquarix</td>\n",
       "      <td>Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Water, nan, 32, 24, 29, 43, 34, 39, 1, False, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Darklurk</td>\n",
       "      <td>Dark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dark, nan, 45, 28, 22, 35, 27, 71, 5, False, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flametrix</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>101</td>\n",
       "      <td>80</td>\n",
       "      <td>135</td>\n",
       "      <td>129</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Fire, nan, 80, 101, 80, 135, 129, 64, 1, False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Punchopponent</td>\n",
       "      <td>Fighting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>74</td>\n",
       "      <td>43</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fighting, nan, 57, 74, 43, 71, 71, 79, 3, Fals...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name_open_ai    Type 1  Type 2  HP  Attack  Defense  Sp. Atk  Sp. Def  \\\n",
       "0       Dragound    Ground  Dragon  81      93       99       88       83   \n",
       "1        Aquarix     Water     NaN  32      24       29       43       34   \n",
       "2       Darklurk      Dark     NaN  45      28       22       35       27   \n",
       "3      Flametrix      Fire     NaN  80     101       80      135      129   \n",
       "4  Punchopponent  Fighting     NaN  57      74       43       71       71   \n",
       "\n",
       "   Speed  Generation  Legendary  Evolution  \\\n",
       "0     71           6      False        0.0   \n",
       "1     39           1      False        1.0   \n",
       "2     71           5      False        1.0   \n",
       "3     64           1      False        2.0   \n",
       "4     79           3      False        1.0   \n",
       "\n",
       "                                            Attribut  \n",
       "0  Ground, Dragon, 81, 93, 99, 88, 83, 71, 6, Fal...  \n",
       "1  Water, nan, 32, 24, 29, 43, 34, 39, 1, False, 1.0  \n",
       "2   Dark, nan, 45, 28, 22, 35, 27, 71, 5, False, 1.0  \n",
       "3  Fire, nan, 80, 101, 80, 135, 129, 64, 1, False...  \n",
       "4  Fighting, nan, 57, 74, 43, 71, 71, 79, 3, Fals...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das funktioniert leider überhaupt nicht gut, teilweise generiert er keinen Namen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llm/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '10', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      15.09 ms /    32 runs   (    0.47 ms per token,  2120.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33739.79 ms /   295 tokens (  114.37 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time =    8566.89 ms /    31 runs   (  276.35 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =   42437.34 ms /   326 tokens\n"
     ]
    }
   ],
   "source": [
    "model_path = \"llm/llama-2-7b-chat.Q2_K.gguf\"\n",
    "llm = Llama(model_path=model_path)\n",
    "system_message = \"Create a non-existent Pokemon name based on these statistics: Stats: Name, Type 1, Type 2, Total, HP, Attack, Defence, Sp. Atk, Sp. Def, Speed, Generation, Legendary, Evolution // Examples: Input: Grass, Poison, 318, 45, 49, 49, 65, 65, 45, 1, False, 0; Output: Bulbasaur Input: Fire, NaN, 309, 39, 52, 43, 60, 50, 65 1 False, 1; Output: Charmander Input: Fire, Water, 600, 80, 110, 120, 130, 90, 70, 6, True, 0; Output: Volcanic Ion; Give only the Name as Output!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_2_pokemon_assistant(pokemon_stats, llm=llm, system_message=system_message):\n",
    "    user_message = f\"Give me a pokemon name for these stats: {pokemon_stats}\"\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>> Create a non-existent Pokémon name based on the provided statistics. The statistics format includes: Name, Type 1, Type 2, Total, HP, Attack, Defence, Sp. Atk, Sp. Def, Speed, Generation, Legendary, Evolution. Examples of inputs and outputs are as follows:\n",
    "- Input: Grass, Poison, 318, 45, 49, 49, 65, 65, 45, 1, False, 0; Output: Bulbasaur\n",
    "- Input: Fire, NaN, 309, 39, 52, 43, 60, 50, 65, 1, False, 1; Output: Charmander\n",
    "- Input: Fire, Water, 600, 80, 110, 120, 130, 90, 70, 6, True, 0; Output: Volcanic Ion [/SYS] [/INST]\n",
    "[INST] Input: {user_message} Output: [/INST]</s>\"\"\"\n",
    "    output = llm(\n",
    "    prompt,\n",
    "    max_tokens=40, # Generate up to 32 tokens\n",
    "    #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "    echo=True # Echo the prompt back in the output\n",
    "    ) \n",
    "    return output[\"choices\"][0][\"text\"].split(\"]\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      22.25 ms /    40 runs   (    0.56 ms per token,  1797.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43049.42 ms /   299 tokens (  143.98 ms per token,     6.95 tokens per second)\n",
      "llama_print_timings:        eval time =   13801.11 ms /    39 runs   (  353.87 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   57036.86 ms /   338 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      20.96 ms /    40 runs   (    0.52 ms per token,  1907.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10782.22 ms /    44 tokens (  245.05 ms per token,     4.08 tokens per second)\n",
      "llama_print_timings:        eval time =   13046.01 ms /    39 runs   (  334.51 ms per token,     2.99 tokens per second)\n",
      "llama_print_timings:       total time =   24004.98 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      17.42 ms /    40 runs   (    0.44 ms per token,  2295.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10351.60 ms /    44 tokens (  235.26 ms per token,     4.25 tokens per second)\n",
      "llama_print_timings:        eval time =   10487.03 ms /    39 runs   (  268.90 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:       total time =   20999.27 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      19.57 ms /    40 runs   (    0.49 ms per token,  2043.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11120.64 ms /    47 tokens (  236.61 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:        eval time =   10763.54 ms /    39 runs   (  275.99 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =   22048.25 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      20.55 ms /    40 runs   (    0.51 ms per token,  1946.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10769.11 ms /    45 tokens (  239.31 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:        eval time =   11070.27 ms /    39 runs   (  283.85 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:       total time =   22007.22 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      20.12 ms /    40 runs   (    0.50 ms per token,  1987.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11133.50 ms /    46 tokens (  242.03 ms per token,     4.13 tokens per second)\n",
      "llama_print_timings:        eval time =   11381.51 ms /    39 runs   (  291.83 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =   22680.61 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      20.68 ms /    40 runs   (    0.52 ms per token,  1933.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9186.32 ms /    44 tokens (  208.78 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:        eval time =    9921.65 ms /    39 runs   (  254.40 ms per token,     3.93 tokens per second)\n",
      "llama_print_timings:       total time =   19279.49 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      19.61 ms /    40 runs   (    0.49 ms per token,  2039.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11134.21 ms /    47 tokens (  236.90 ms per token,     4.22 tokens per second)\n",
      "llama_print_timings:        eval time =    9293.13 ms /    39 runs   (  238.29 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:       total time =   20587.46 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      19.03 ms /    40 runs   (    0.48 ms per token,  2102.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9230.97 ms /    44 tokens (  209.79 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:        eval time =    9419.69 ms /    39 runs   (  241.53 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:       total time =   18810.20 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33740.20 ms\n",
      "llama_print_timings:      sample time =      20.96 ms /    40 runs   (    0.52 ms per token,  1908.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10931.25 ms /    46 tokens (  237.64 ms per token,     4.21 tokens per second)\n",
      "llama_print_timings:        eval time =   12610.99 ms /    39 runs   (  323.36 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =   23731.72 ms /    85 tokens\n"
     ]
    }
   ],
   "source": [
    "syn_data['Name_llama_2'] = syn_data[\"Attribut\"].apply(llama_2_pokemon_assistant) # Assistant wird für jeden Datenpunkt (Pokemon) angewendet.\n",
    "columns = ['Name_llama_2'] + [col for col in syn_data.columns if col != 'Name_llama_2'] # Name sollte ganz vorne stehen :)\n",
    "syn_data = syn_data[columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
